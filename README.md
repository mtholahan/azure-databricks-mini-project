# Azure Mini Project 01: Databricks Lab

**Springboard Data Engineering Boot Camp**  
Author: Mark Holahan  
Project Type: Mini Project  
Platform: Azure Databricks  
Tools: PySpark, Databricks, Spark DataFrames

---

## ðŸ“˜ Overview

This project demonstrates the use of **Azure Databricks** and **native PySpark syntax** to ingest, clean, and analyze structured banking datasets. The exercise was completed as part of the Springboard Data Engineering Boot Camp and reinforces foundational big data processing techniques using Apache Spark.

---

## ðŸ› ï¸ Technologies Used

| Tool/Platform    | Purpose                                |
| ---------------- | -------------------------------------- |
| Azure Databricks | Interactive notebooks + compute engine |
| PySpark          | Data transformation and querying       |
| Apache Spark     | Distributed data processing            |
| VS Code          | Local editing and QA                   |
| Git + GitHub     | Version control and project tracking   |

---

## ðŸ“‚ Project Structure

```bash
azure-mini-project-01/
â”œâ”€â”€ banking_lab.ipynb        # Main notebook with all PySpark queries
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loan.csv
â”‚   â”œâ”€â”€ credit card.csv
â”‚   â””â”€â”€ txn.csv
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```



## ðŸ“Š Datasets Used

Three CSV files were used to simulate a multi-domain banking environment:

- `loan.csv`: Customer loan details (e.g., income, loan category, cheque return, etc.)
- `credit card.csv`: Credit eligibility, salary, and account metadata
- `txn.csv`: Transaction-level data including deposits and withdrawals



## âœ… Goals

Each dataset was used to answer specific business questions using **PySpark**:

- Schema introspection (`.printSchema()`)
- Filtering based on conditions (`.filter()`)
- Aggregations (`.agg()`, `.groupBy()`)
- Distinct record counts
- Customer segmentation (by income, location, activity, etc.)

All logic was implemented using PySpark DataFrame syntax, not SQL.

## ðŸ” Sample Queries

```python
# Number of customers with income > 60,000
loan_df.filter(col("Income") > 60000).count()

# Maximum withdrawal per account
txn_df.groupBy("ACCOUNT NO").agg(max("WITHDRAWAL AMT")).show()
```

## ðŸ“ˆ Summary

All analytical prompts were completed using native PySpark syntax.
The notebook reflects clean, well-structured, and readable logic with verified outputs.
Compute was shut down after use to minimize costs.



## ðŸ“Œ Notes

- All queries were validated by manual re-run.
- GitHub Copilot suggestions were reviewed and confirmed for accuracy.
- The notebook was QA'd offline using VS Code's native Jupyter support.

## ðŸ“Ž Related Repositories

This repo is part of a larger meta-repo:

ðŸ”— [capstone-meta-repo](https://github.com/mtholahan/Springboard-Projects) *(replace with actual URL)*
